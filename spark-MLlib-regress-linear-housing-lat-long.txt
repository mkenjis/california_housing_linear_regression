-------------------------- Steps in spark-shell to generate a shell script to query a webservice to get location from latitude/longitude 

val rdd = sc.textFile("housing/cal_housing.data").map(x => x.split(","))

val rdd1 = rdd.map(x => (x(1),x(0))).distinct

rdd1.map( x => "curl \"http://www.geoplugin.net/extras/location.gp?lat="+x._1+"&lon="+x._2+"\"; echo "; "+x._1+"; "+x._2).saveAsTextFile("/tmp/latlong")

-------------------------- Step to run in Unix

> cp /tmp/latlong/part-00000 latlong.sh
> chmod u+x latlong.sh
nohup latlong.sh >latlong1.txt 2>/dev/null &

-------------------------- Steps to join cal_housing.data with latlong1.txt (contains California suburb of latitude/longitude) 

val rdd = sc.textFile("latlong1.txt").map(x => x.split(";"))

rdd.map( x => (x(19).trim()+":"+x(20).trim())).first

rdd.map( x => (x(19).trim()+":"+x(20).trim(),x(1),x(5))).first

val rdd2 = rdd.map( x => (x(19).trim()+":"+x(20).trim(),x(1),x(5)))

rdd2.map(x => x._1).distinct.count

rdd2.map(x => x._1).count

rdd.map(x => x.length).distinct.count

rdd.map(x => x.length).distinct.take(10)

rdd.filter(x => x.length == 4).take(10)

rdd.filter(x => x.length == 4).count

val rdd2 = rdd.map( x => {
  if (x.length == 4) (x(2).trim()+":"+x(3).trim(),Array("",""))
  else (x(19).trim()+":"+x(20).trim(),Array(x(1),x(5)))
})

rdd2.map( x => x._1 ).distinct.count

val hrdd = sc.textFile("housing/cal_housing.data").map(x => x.split(","))

val hrdd1 = hrdd.map( x => (x(1)+":"+x(0),x))

val frdd = hrdd1.join(rdd2)

val frdd1 = frdd.map( x => Array(x._2._2(0),x._2._2(1),x._2._1(0),x._2._1(1),x._2._1(2),x._2._1(3),x._2._1(4),x._2._1(5),x._2._1(6),x._2._1(7),x._2._1(8)))

frdd1.map(x => x(0)+","+x(1)+","+x(2)+","+x(3)+","+x(4)+","+x(5)+","+x(6)+","+x(7)+","+x(8)+","+x(9)+","+x(10)).saveAsTextFile("/tmp/house_city")

-------------------------- Analysis and transformation of housing dataset using location instead of Latitude/Longitude 

val rdd = sc.textFile("housing-city.txt").map( x => x.split(","))

val rdd1 = rdd.map( x => Array(x(2).toDouble,x(3).toDouble,x(4).toDouble,x(5).toDouble,x(6).toDouble,x(7).toDouble,x(8).toDouble,x(9).toDouble,x(10).toDouble))

def oneHotEncColumns(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int):org.apache.spark.rdd.RDD[Array[Double]] = {
  val categories = rddx.map(r => r(idx)).distinct.zipWithIndex.collect.toMap
  val numCategories = categories.size
  val vetcateg = rddx.map(r => {
      val categoryIdx = categories(r(idx)).toInt
      val categoryFeatures = Array.ofDim[Double](numCategories)
      categoryFeatures(categoryIdx) = 1.0
      categoryFeatures
  })
  vetcateg
}

def mergeArray(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int*):org.apache.spark.rdd.RDD[Array[Double]] = {
  var i = 0
  var arr1 = oneHotEncColumns(rddx,idx(i))
  for (j <- 1 until idx.size) {
    var arr2 = oneHotEncColumns(rddx,idx(j))
    var flt1 = arr1.zip(arr2).map(x => (x._1.toList ++ x._2.toList).toArray)
    arr1 = flt1
  }
  arr1
}

val concat = mergeArray(rdd,0)

val vect = concat.zip(rdd1).map(x => (x._1.toList ++ x._2.toList).toArray)

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

val data = vect.map(r => {
  val arr_size = r.size
  val l = r(arr_size-1)/10000
  val f = r.slice(0,arr_size-2)
  LabeledPoint(l,Vectors.dense(f))
})

val sets = data.randomSplit(Array(0.8,0.2))
val trainSet = sets(0)
val testSet = sets(1)

import org.apache.spark.mllib.regression.LinearRegressionWithSGD
val model = LinearRegressionWithSGD.train(trainSet, 200, 1.0)
----
import org.apache.spark.mllib.regression.LinearRegressionWithSGD
val alg = new LinearRegressionWithSGD()
alg.setIntercept(true)
alg.optimizer.setNumIterations(200)

---- step size = 1.0
alg.optimizer.setStepSize(1.0)
val model = alg.run(trainSet)
model: intercept = NaN, numFeatures = 818

---- step size = 0.1
alg.optimizer.setStepSize(0.1)
val model = alg.run(trainSet)
model: intercept = NaN, numFeatures = 818

---- step size = 0.01
alg.optimizer.setStepSize(0.01)
val model = alg.run(trainSet)
model: intercept = NaN, numFeatures = 818

---- step size = 0.001
alg.optimizer.setStepSize(0.001)
val model = alg.run(trainSet)
model: intercept = NaN, numFeatures = 818

---- step size = 0.0001
alg.optimizer.setStepSize(0.0001)
val model = alg.run(trainSet)
model: intercept = NaN, numFeatures = 818

---- step size = 0.00001
alg.optimizer.setStepSize(0.00001)
val model = alg.run(trainSet)
model: intercept = -2.0558288956040616E237, numFeatures = 818

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(10)
res15: Array[(Double, Double)] = Array((-2.805889351345187E244,8.88), (-2.3631147919032612E244,15.92), (-4.368854206321573E244,26.67), (-2.990192356304651E244,10.0), (-3.5671067109101555E244,23.58), (-4.0663905436860975E244,9.75), (-2.2807996129677395E244,18.16), (-3.2271943607502757E244,19.96), (-6.856912232975341E244,31.86), (-2.7001253679789552E244,7.62))

---- step size = 0.000001
alg.optimizer.setStepSize(0.000001)
val model = alg.run(trainSet)
model: intercept = 0.988456152954546, numFeatures = 818

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(10)
res17: Array[(Double, Double)] = Array((-3212.7263361818964,8.88), (4527.63674982092,15.92), (-41196.37706795759,26.67), (5594.492640751218,10.0), (4975.775588629796,23.58), (77.55545389388203,9.75), (2338.2554284315534,18.16), (-4053.590731983208,19.96), (-30470.45411834553,31.86), (7956.440092976349,7.62))

---- step size = 0.0000001
alg.optimizer.setStepSize(0.0000001)
val model = alg.run(trainSet)
model: intercept = 1.0000023959572417, numFeatures = 818

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(10)
res19: Array[(Double, Double)] = Array((12.46567609946326,8.88), (10.540473625047694,15.92), (18.844834609519307,26.67), (13.112720847341867,10.0), (15.553389693679733,23.58), (17.638692638079455,9.75), (10.325704824959434,18.16), (14.133015851207674,19.96), (28.994072033475675,31.86), (12.001896070514562,7.62))

import org.apache.spark.mllib.evaluation.RegressionMetrics
val validMetrics = new RegressionMetrics(validPredicts)
validMetrics.rootMeanSquaredError  // 16.821539165158427
validMetrics.meanSquaredError  // 282.96417988495887

---- step size = 0.00000001
alg.optimizer.setStepSize(0.00000001)
val model = alg.run(trainSet)
model: intercept = 1.0000003213058992, numFeatures = 818

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(10)
res23: Array[(Double, Double)] = Array((3.798283596193442,8.88), (3.347893652176225,15.92), (5.356434150762241,26.67), (3.9740281784398803,10.0), (4.555717956858309,23.58), (5.057078040700263,9.75), (3.2750458105663167,18.16), (4.2143250033248405,19.96), (7.836440719968052,31.86), (3.6904093582524338,7.62))

import org.apache.spark.mllib.evaluation.RegressionMetrics
val validMetrics = new RegressionMetrics(validPredicts)
validMetrics.rootMeanSquaredError  // 20.43061974055267
validMetrics.meanSquaredError  // 417.4102229830606


-------------------------- Decide to scale features because variabiliaty increases even reducing step size of model 

import org.apache.spark.mllib.feature.StandardScaler
val scaler = new StandardScaler(true, true).fit(trainSet.map(x => x.features))
 
val trainScaled = trainSet.map(x => LabeledPoint(x.label,scaler.transform(x.features)))
val validScaled = testSet.map(x => LabeledPoint(x.label, scaler.transform(x.features)))

import org.apache.spark.mllib.regression.LinearRegressionWithSGD
val alg = new LinearRegressionWithSGD()
alg.setIntercept(true)
alg.optimizer.setNumIterations(200)

---- step size = 1.0
alg.optimizer.setStepSize(1.0)
val model = alg.run(trainScaled)
model: intercept = 20.628720088340465, numFeatures = 818

val validPredicts = validScaled.map(x => (model.predict(x.features),x.label))

validPredicts.take(10)
res31: Array[(Double, Double)] = Array((10.399655510703083,8.88), (18.878225530969242,15.92), (24.257563127084538,26.67), (10.407386534869058,10.0), (30.719835827322207,23.58), (11.165803988038578,9.75), (20.000718874242075,18.16), (20.6901955029934,19.96), (26.09321111976859,31.86), (7.990721657493841,7.62))

import org.apache.spark.mllib.evaluation.RegressionMetrics
val validMetrics = new RegressionMetrics(validPredicts)
validMetrics.rootMeanSquaredError  // 6.312778659011311
validMetrics.meanSquaredError  // 39.85117439766864

---- step size = 0.1
alg.optimizer.setStepSize(0.1)
val model = alg.run(trainScaled)
model: intercept = 18.548671974787677, numFeatures = 818

val validPredicts = validScaled.map(x => (model.predict(x.features),x.label))

validPredicts.take(10)
res35: Array[(Double, Double)] = Array((9.311304144345204,8.88), (17.42472356477238,15.92), (22.0187559548075,26.67), (9.302662673272952,10.0), (28.35677977142204,23.58), (8.934442351228897,9.75), (17.5527841169764,18.16), (19.39909457603021,19.96), (22.48648764612915,31.86), (5.697034095026574,7.62))

import org.apache.spark.mllib.evaluation.RegressionMetrics
val validMetrics = new RegressionMetrics(validPredicts)
validMetrics.rootMeanSquaredError  // 6.998520055720346
validMetrics.meanSquaredError  // 48.97928297031991

---- step size = 0.01
alg.optimizer.setStepSize(0.01)
val model = alg.run(trainScaled)
model: intercept = 5.6278755586280385, numFeatures = 818

val validPredicts = validScaled.map(x => (model.predict(x.features),x.label))

validPredicts.take(10)
res39: Array[(Double, Double)] = Array((3.1659883767360655,8.88), (5.4387117179454405,15.92), (6.9759188606084495,26.67), (2.9530434430910137,10.0), (8.371814052161627,23.58), (2.6250555778995883,9.75), (5.195252628408679,18.16), (6.12695708778405,19.96), (7.285468326170644,31.86), (1.7796777685738463,7.62))

import org.apache.spark.mllib.evaluation.RegressionMetrics
val validMetrics = new RegressionMetrics(validPredicts)
validMetrics.rootMeanSquaredError  // 18.07999257626309
validMetrics.meanSquaredError  // 326.88613155772845